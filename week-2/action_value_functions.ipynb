{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5a88ff",
   "metadata": {},
   "source": [
    "### **Actions_value_functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5bd20",
   "metadata": {},
   "source": [
    "**Q-values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97beed",
   "metadata": {},
   "source": [
    "* Expected return of:\n",
    "    - Starting at a state s\n",
    "    - Taking a action\n",
    "    - Following a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b359b",
   "metadata": {},
   "source": [
    "Action value functions also known as Q-values provide us with an estimate of the expected return of starting in a state, taking a certain action, and then following a policy thereafter. Therefore, the action value is the sum of the immediate reward received after performing an action and the discounted value of the new state computed for a specific policy. While state-value functions give us a broad overview of the desirability of states, action-value functions break it down further, giving us insight into the desirability of actions within those states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc2afa",
   "metadata": {},
   "source": [
    "$Q(s,a)=r_{a}+\\gamma V(s+1)$\\\n",
    "Action-value of state a,action a &rarr; sum of:\n",
    "- reward received after performing action a in state s\n",
    "- discounted value of next state resulting from action a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7c1d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
