{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5a88ff",
   "metadata": {},
   "source": [
    "### **Actions_value_functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5bd20",
   "metadata": {},
   "source": [
    "**Q-values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97beed",
   "metadata": {},
   "source": [
    "* Expected return of:\n",
    "    - Starting at a state s\n",
    "    - Taking a action\n",
    "    - Following a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b359b",
   "metadata": {},
   "source": [
    "Action value functions also known as Q-values provide us with an estimate of the expected return of starting in a state, taking a certain action, and then following a policy thereafter. Therefore, the action value is the sum of the immediate reward received after performing an action and the discounted value of the new state computed for a specific policy. While state-value functions give us a broad overview of the desirability of states, action-value functions break it down further, giving us insight into the desirability of actions within those states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc2afa",
   "metadata": {},
   "source": [
    "$Q(s,a)=r_{a}+\\gamma V(s+1)$\\\n",
    "Action-value of state a,action a &rarr; sum of:\n",
    "- reward received after performing action a in state s\n",
    "- discounted value of next state resulting from action a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7c1d5",
   "metadata": {},
   "source": [
    "**Grid World**\\\n",
    "Recall the nine states and the policy dictating the agent's deterministic movements. We previously evaluated this policy using state-value functions. Now we need to compute action values for each state, which means that, for each state, we have to compute 4 values. We'll keep the state values on the right as we will need them for the action-value computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae670ea1",
   "metadata": {},
   "source": [
    "![image](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd8556",
   "metadata": {},
   "source": [
    "**Q-values - state 4**\\\n",
    "Suppose the agent is born in the state 4.\\\n",
    "The agent can choose to go in four direction.\\\n",
    "If agent moves down from State 4, it receives a -2 reward and lands in state 5 having a value of 5 which we have previously calculated.\\\n",
    "**State 4 - action down**\n",
    "The Q-value for moving down from state 4 combines a reward of -2 with the next state's value, which gives 3, assuming a discount factor of 1.\\\n",
    "**State 4-action left**\\\n",
    "Moving left yields a Q-value of 1, calculated by adding a reward of -1 to the value of the resulting state, which is 2.\\\n",
    "**State 4-action up**\\\n",
    "Similarly, moving up results in a Q-value of 7, derived from a -1 reward and the new state's value of 8.\\\n",
    "**State 4-action right**\\\n",
    "Finally, when the agent moves right, it receives a reward of -1 and visits a state of value 10, leading to a Q-value of 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7acc981",
   "metadata": {},
   "source": [
    "The process is repeated for all the states untill all the q values have in computed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2e23e2",
   "metadata": {},
   "source": [
    "```\n",
    "def compute_q_value(state,action):\n",
    "    if state == terminal_state:\n",
    "        return None\n",
    "    _,next_state,reward,_,_=env.unwrapped.P[state][action][0]\n",
    "    return reward +gamma*compute_state_value(next_state)\n",
    "\n",
    "Q={(state,action):compute_q_values(state,action)\n",
    "    for state in range(num_states)\n",
    "    for action in range(num_staes)}\n",
    "print(Q)\n",
    "```\n",
    "Remember that we are taking the code of the previous lectures as the left part of the code such as the compute_state_value(state)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c39f0",
   "metadata": {},
   "source": [
    "Now we can improve our policy based upon which action has the best q value for the given state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc492ea",
   "metadata": {},
   "source": [
    "```\n",
    "improved_policy={}\n",
    "for state in range(num_states-1):\n",
    "    max_action=max(range(num_actions),key=lambda action:Q[(state,action)])#this is first we define the actions through range(num action ) then key to find maximum is lamda which for the action in the range gives the tuple(state,action) in the dictionary Q which will return Q value then we select the action based on the max value\n",
    "    improved_policy[state]=action\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c6e39",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
