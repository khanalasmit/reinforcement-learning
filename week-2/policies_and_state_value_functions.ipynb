{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fd4df9",
   "metadata": {},
   "source": [
    "### **Policies and State Value Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e59f1",
   "metadata": {},
   "source": [
    "RL objective &rarr; Formulate effective policies.\\\n",
    "Specify which action to take in each state to return maximize return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a9c76",
   "metadata": {},
   "source": [
    "### **Consider Grid World Example**\n",
    "* Agent aims to reach diamond while avoding mountains.\n",
    "* Nine states.\n",
    "* Deterministic movements.\\\n",
    "**Grid World example-rewards**\n",
    "* Given based on states:\n",
    "    - Diamond: +10\n",
    "    - Mountains: -2\n",
    "    - Other states: -1\n",
    "\n",
    "**Grid world example-Policy**\n",
    "```\n",
    "    0: left, 1:down, 2: right,3 :up\\\n",
    "    policy={\n",
    "        0:1,1:2,2:1,3:1,4:3,5:1,6:2,7:3\n",
    "    }\n",
    "    #intialize the environment\n",
    "    state,info=env.reset()\n",
    "    terminated=False\n",
    "    while not terminated:\n",
    "        action=policy[state]\n",
    "        state,reward,terminated,_,_ =env.step(action)\n",
    "```\n",
    "\n",
    "**To valuate the policy we utiliz state value functions**\n",
    "* Estimates the states worth\n",
    "* Expected return starting from state, following policy\\\n",
    "$V(S)=r_{s+1}+\\gamma r_{s+2}+\\gamma^2 r_{s+3}+...+\\gamma^{n-1} r_{s+n}$\\\n",
    "Sum of discounted rewards collected by\n",
    "- starting in states\n",
    "- and following the policy\n",
    "- This involves discounting reward by a factor, gamma ,over time, and summing these discounted rewards.\\\n",
    "\\\n",
    "**Grid world example:State-values**\\\n",
    "In our example, we have 9 states, therefore, we need to compute nine state values. For simplicity we consider a discount factor gamma of 1.\\\n",
    "\\\n",
    "**Value of Goal state**\n",
    "- starting in goal state, agent doesnot move\n",
    "- V(goal state)=0\n",
    "- starting in state 5, agent moves to goal\n",
    "- V(5)=10\n",
    "- staring in 2, rewards are -1,10.\n",
    "- And so on untill all the states values are computed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008ddfc",
   "metadata": {},
   "source": [
    "**Bellman equation**\\\n",
    "In practice, the Bellman equation, a recursive formula, computes state values by combining the immediate reward of the current state with the discounted value of the next state, thereby connecting each state's value to its successors. In deterministic environments like ours, this standard formula suffices, whereas non-deterministic environments require modifications to incorporate transition probabilities.\\\n",
    "$V(S)=r_{s+1}+\\gamma V(s+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2961455",
   "metadata": {},
   "source": [
    "### **Computing state-values**\n",
    "```\n",
    "def compute_state_value(state):\n",
    "        if state==terminal_state:\n",
    "            return 0\n",
    "        action=policy[state]\n",
    "        _,next_state,reward,_,_=env.unwrapped.P[state][action]\n",
    "        return reward+gamma*compute_state_value(next_state)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425f7f22",
   "metadata": {},
   "source": [
    "```\n",
    "terminal_state=8\n",
    "gamma=1\n",
    "V={state:compute_state_value for states in range(num_states)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c6787",
   "metadata": {},
   "source": [
    "To compare we define new policy and check which have high state value we keep that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a0444",
   "metadata": {},
   "source": [
    "```\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b32f4",
   "metadata": {},
   "source": [
    "```\n",
    "def render():\n",
    "    state_image=env.render()\n",
    "    plt.imshow(state_image)\n",
    "    plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e8c0cb",
   "metadata": {},
   "source": [
    "```\n",
    "# Create the environment\n",
    "env = gym.make('MyGridWorld', render_mode='rgb_array')\n",
    "state, info = env.reset()\n",
    "\n",
    "# Define the policy\n",
    "policy = {0:2, 1:2, 2:1, 3:1, 4:0, 5:0, 6:2, 7:2}\n",
    "\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    # Select action based on policy \n",
    "    action = policy[state]\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    # Render the environment\n",
    "    render()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b9437",
   "metadata": {},
   "source": [
    "```\n",
    "improved_policy = {}\n",
    "\n",
    "for state in range(num_states-1):\n",
    "    # Find the best action for each state based on Q-values\n",
    "    max_action = max(range(num_actions), key=lambda action:Q[(state,action)])\n",
    "    improved_policy[state] = max_action\n",
    "\n",
    "terminated = False\n",
    "state=0\n",
    "while not terminated:\n",
    "  # Select action based on policy \n",
    "  action = improved_policy[state]\n",
    "  # Execute the action\n",
    "  state, reward, terminated, truncated, info = env.step(action)\n",
    "  render()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b37d8d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
