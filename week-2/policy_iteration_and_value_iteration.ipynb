{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e9ac50",
   "metadata": {},
   "source": [
    "## **Policy and Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3e0bb",
   "metadata": {},
   "source": [
    "**Policy iteration** is an iterative process to find the optimal policy.\\\n",
    "First we intialze a policy &rarr; then we evaluate by computing state value &rarr; then improve the policy\\\n",
    "This evaluate imporve cycle continues untill the policy stablaizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f5a0d",
   "metadata": {},
   "source": [
    "**Code example of the grid world** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690df15c",
   "metadata": {},
   "source": [
    "```\n",
    "def policy_evaluation(policy):\n",
    "    V={state:compute_state_vlaue(state,policy) for state in range(num_states)}\n",
    "    return V\n",
    "```\n",
    "**policy improvement**\n",
    "```\n",
    "def policy_improvement(policy):\n",
    "    improved_policy={s:0 for s in range(num_states-1)}\n",
    "    Q={(state,action):compute_q_value(state,action,policy) for state in range(num_states) for action in range(num_actions)}\n",
    "\n",
    "    for state in range(num_states-1):\n",
    "        max_action=max(range(num_actions), key=lambda action:Q[(state,action)])\n",
    "        improved_policy[state]=max_action\n",
    "    return improved_policy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a0103",
   "metadata": {},
   "source": [
    "Policy iteration begins by initializing the policy we want to optimize\\\n",
    "```\n",
    "def policy_iteration():\n",
    "    policy={0:1,1:2,2:1,3:1,4:3,5:1,6:2,7:3}\n",
    "    while True:\n",
    "        V=policy_evaluation(policy)\n",
    "        improved_policy=policy_improvement(policy)\n",
    "        if improve_policy==policy:\n",
    "            break\n",
    "        policy=imporved_policy\n",
    "    return policy,V\n",
    "\n",
    "policy,V=policy_iteration()\n",
    "print(policy,V)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef00f7c",
   "metadata": {},
   "source": [
    "**value iteration**\n",
    "\n",
    "* speeds up the process by combining policy evaluation and policy improvement\n",
    "    - computes optimal state_value function\n",
    "    - Derives policy from it\n",
    "```\n",
    "V={state:0 for state in range(num_states)}\n",
    "policy={state:0 for state in range(num_states-1)}\n",
    "thershold=0.001\n",
    "while True:\n",
    "    new_V={state:0 for state in range(num_states)}\n",
    "    for state in range(num_states-1):\n",
    "        max_action,max_q_value=get_max_action_and_value(state,V)\n",
    "        new_V[state]=max_q_value\n",
    "        policy[state]=max_action\n",
    "        if all(abs(new_v[state]-V[state])< thresh for state in V):\n",
    "            break\n",
    "        V=new_V\n",
    "def get_max_action_and_value(state,V):\n",
    "    Q_values=[compute_q_value(state,action,V) for action in range(num_actions)]\n",
    "    max_action=max(range(num_actions),key=lambda a:Q_values[a])\n",
    "    max_q_values=Q_values[max_action]\n",
    "    return max_action,max_q_value\n",
    "\n",
    "def compute_q_value(state,action,V):\n",
    "    if sate=terminal_state:\n",
    "        return None\n",
    "    _,next_state,reward,_=env.P[state][action][0]\n",
    "    return reward+gamma*V[next_state]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05400d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cea6d4e0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
