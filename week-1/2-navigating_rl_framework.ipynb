{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae71041d",
   "metadata": {},
   "source": [
    "# **Navigating the RL framework**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55de619",
   "metadata": {},
   "source": [
    "![image](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c9142",
   "metadata": {},
   "source": [
    "![image](2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b71bc",
   "metadata": {},
   "source": [
    "## **Epsiodic vs Continous tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ddd2e",
   "metadata": {},
   "source": [
    "* **Episodic task**\n",
    "    - Tasks segmented in episodes\n",
    "    - Episode has begining and end\n",
    "    - **Example:** agent playing chess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf6c74",
   "metadata": {},
   "source": [
    "* Continous task\n",
    "    - Contionous interaction\n",
    "    - No distinct episodes\n",
    "    - **Example:** Adjusted traffic lights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84086006",
   "metadata": {},
   "source": [
    "## **Return**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d738a3",
   "metadata": {},
   "source": [
    "* Actions have long term consequences\n",
    "* Agents aims to maximize the total reward over time\n",
    "* Return:  sum of all the expected rewards\\\n",
    "$Return: r_1+r_2+r_3+r_4+...+r_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b7c58",
   "metadata": {},
   "source": [
    "## **Discounted Return**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3dbfd",
   "metadata": {},
   "source": [
    "* Immediate rewards are more valuable than future ones\n",
    "* **Discounted return:**  gives more weight to nearer rewards\n",
    "* Discount factor $ \\gamma $: prioratize present rewards\n",
    "* $Discounted rewared: r_1+\\gamma r_2+\\gamma^2 r_3+...+\\gamma^{n-1}r_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e518a3d",
   "metadata": {},
   "source": [
    "If $ \\gamma$ is zero than it favours only immediate gains\\\n",
    "If $ \\gamma$ is one than it favours future gains without discounts.\\\n",
    "* Lower gain &rarr; immediate gains\n",
    "* Higher gain &rarr; long-term benifit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3c534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9419cacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The discounted return of the second strategy is -2.3880000000000003\n"
     ]
    }
   ],
   "source": [
    "exp_rewards_strategy_2 = np.array([6, -5, -3, -2])\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "# Compute discounts\n",
    "discounts_strategy_2 = np.array([discount_factor**i for i in range(len(exp_rewards_strategy_2))])\n",
    "\n",
    "# Compute the discounted return\n",
    "discounted_return_strategy_2 = np.sum(exp_rewards_strategy_2*discounts_strategy_2)\n",
    "\n",
    "print(f\"The discounted return of the second strategy is {discounted_return_strategy_2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
