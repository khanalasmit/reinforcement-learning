{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee79f41b",
   "metadata": {},
   "source": [
    "## **DOUBLE Q-LEARNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe02626",
   "metadata": {},
   "source": [
    "### **Q-Learning**\n",
    "* Estimates optimal action-values function\n",
    "* It has tendency to overestimates by updating based on max Q.\n",
    "* Might learn to suboptimal policy learning(especially in the environments with noisy and stochastic rewards).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c15ce",
   "metadata": {},
   "source": [
    "### **Double Q-Learning**\n",
    "* Maintains two Q tables.\n",
    "* It maintains Q0 and Q1 . Each table is updated using information from other, thus reducing the risk of overestimating Q-values.\\\n",
    "*The key insight behind Double Q-learning is that by splitting the maximization step between two tables, we obtain a more accurate estimate of the action-value function.*\\\n",
    "**To update the Q-value for the chosen action:**\n",
    "* Randomly select a table.\\\n",
    "![image](3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21956c57",
   "metadata": {},
   "source": [
    "Let's say it picks Q0. It then uses Q0 to determine the best next action but updates its value based on the reward observed and the estimated value of the next action from Q1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59bf5c7",
   "metadata": {},
   "source": [
    "$$\n",
    "Q_o(s,a)=(1-\\alpha)Q_o(s,a)+\\alpha[r+\\gamma Q_1(s',max_{a})]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f6993",
   "metadata": {},
   "source": [
    "If it picks Q1, it uses Q1 to determine the best next action but it updates the Q-value based on the reward observed and the estimated value of the next action from Q0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cae53",
   "metadata": {},
   "source": [
    "### **Double q-learning**\n",
    "* reduces overestimates bias\n",
    "* alternates between Q0 and Q1 updates.\n",
    "* ensures both q-tables contribute to learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f4d21",
   "metadata": {},
   "source": [
    "## **Implementation with forzen lake environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e588a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1907c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('FrozenLake-v1',is_slippery=False)\n",
    "num_states=env.observation_space.n\n",
    "num_actions=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18737b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=[np.zeros((num_states,num_actions))]*2#represent our dual estimators\n",
    "num_episodes=1000\n",
    "alpha=0.5#learning rate\n",
    "gamma=0.99#discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a6b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(state,action,reward,next_state):\n",
    "    i=np.random.randint(2)\n",
    "    best_next_action=np.argmax(Q[i][next_state])\n",
    "    Q[i][state,action]=(1-alpha)*Q[i][state,action]+alpha*(reward+gamma*Q[1-i][next_state,best_next_action])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482cde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state,info=env.reset()\n",
    "    terminated=False\n",
    "    while not terminated:\n",
    "        action=np.random.choice(num_actions)\n",
    "        next_state,reward,terminated,truncated,info=env.step(action)\n",
    "        update_q_table(state,action,reward,next_state)\n",
    "        state=next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f51d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: np.int64(2), 1: np.int64(2), 2: np.int64(1), 3: np.int64(0), 4: np.int64(1), 5: np.int64(0), 6: np.int64(1), 7: np.int64(0), 8: np.int64(2), 9: np.int64(2), 10: np.int64(1), 11: np.int64(0), 12: np.int64(0), 13: np.int64(2), 14: np.int64(2), 15: np.int64(0)}\n"
     ]
    }
   ],
   "source": [
    "final_q=(Q[1]+Q[0])/2\n",
    "policy={state:np.argmax(final_q[state]) for state in range(num_states)}\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa497794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
