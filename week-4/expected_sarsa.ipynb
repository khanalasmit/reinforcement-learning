{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c4d3e8",
   "metadata": {},
   "source": [
    "# **EXPECTED SARSA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454463d",
   "metadata": {},
   "source": [
    "It is the temporal difference learning method used in the model free learning. Expected SARSA, like its counterparts SARSA and Q-learning, is a Temporal Difference or TD learning method used in model-free RL, where we start by initializing a Q-table. Then, repeatedly, the agent chooses an action, receives a reward, and updates the table, until convergence is achieved. However, the key distinction of Expected SARSA over SARSA and Q-learning lies in its update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825de40",
   "metadata": {},
   "source": [
    "**Expected SARSA update**\\\n",
    "While SARSA relies on the actual next action taken to update Q-values, and while Q-learning updates Q-values based on the maximum reward attainable from the next state, regardless of the policy being followed, Expected SARSA calculates the expected value of the next state based on all possible actions. This makes Expected SARSA more robust to changes and uncertainties, as it considers the average outcome of all possible next actions according to the current policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfc884",
   "metadata": {},
   "source": [
    "**Expected value of next sate**\\\n",
    "Expected SARSA's formula reflects this approach by focusing on the expected value of the next state. This is achieved by calculating the sum of the Q-values from all possible actions initiated from this state. Each Q-value is weighted by the probability of its corresponding action being selected under the current policy. In our context, since actions are chosen randomly for now when training, it means they have an equal probability of being selected. Therefore, the expected value simplifies to the mean of the Q-values for all actions in the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f725907b",
   "metadata": {},
   "source": [
    "### **Expected value of next state**\n",
    "$$\n",
    "\\text{Q(s,a)} = (1-\\alpha)+\\alpha [r+\\gamma E{Q(s^{'},A)}]\n",
    "$$\n",
    "$$\n",
    "\\text{E{Q(s',A)}}=\\text{Sum}(Prob(a) * Q(s',a)\\text{for a in A})\n",
    "$$\n",
    "For random actions with equal probablities:\n",
    "$$\n",
    "\\text{E{Q(s',A)}}=Mean(Q(s',a)\\text{for a in A})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d93aa8",
   "metadata": {},
   "source": [
    "### **Implementation in Frozen Lake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132f10bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb54f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('FrozenLake-v1',is_slippery=False)\n",
    "num_states=env.observation_space.n\n",
    "num_actions=env.action_space.n\n",
    "Q=np.zeros((num_states,num_actions))\n",
    "gamma=0.99\n",
    "alpha=0.1\n",
    "num_episodes=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0bfed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(state,action,next_state,reward):\n",
    "    expected_q=np.mean(Q[next_state])\n",
    "    Q[state,action]=(1-alpha)*Q[state,action]+alpha*(reward+gamma*expected_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_episodes):\n",
    "    state,info=env.reset()\n",
    "    terminated=False\n",
    "    while not terminated:\n",
    "        action=env.action_space.sample()\n",
    "        next_state,reward,terminated,truncated,info=env.step(action)\n",
    "        update_q_table(state,action,next_state,reward)\n",
    "        state=next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3400797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: np.int64(1), 1: np.int64(2), 2: np.int64(1), 3: np.int64(0), 4: np.int64(1), 5: np.int64(0), 6: np.int64(1), 7: np.int64(0), 8: np.int64(2), 9: np.int64(2), 10: np.int64(1), 11: np.int64(0), 12: np.int64(0), 13: np.int64(2), 14: np.int64(2), 15: np.int64(0)}\n"
     ]
    }
   ],
   "source": [
    "policy={state:np.argmax(Q[state]) for state in range(num_states)}\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e0f625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
