{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e999b5b4",
   "metadata": {},
   "source": [
    "## **Multi Armed Bandits**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfcd882",
   "metadata": {},
   "source": [
    "The Multi-Armed Bandit problem is an analogy to a gambler facing a row of slot machines, each with a different, unknown probability of winning. The challenge is to maximize their winnings by deciding which machine to play, how many times to play it, and when to switch to another machine. This scenario perfectly encapsulates the exploration-exploitation trade-off: exploring to find the machine with the highest reward but exploiting known information to maximize winnings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c8e4b",
   "metadata": {},
   "source": [
    "* challange&rarr;maximize wining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ec94f",
   "metadata": {},
   "source": [
    "To creat a simulated multi armed bandit environment we start by assuming we have multiple slot machines each with own probablity of wining. Each probablity unknown to the agent and will need to learn during training. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
