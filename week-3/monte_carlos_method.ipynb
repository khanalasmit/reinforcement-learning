{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2d8b1d",
   "metadata": {},
   "source": [
    "## **MODEL FREE LEARNING**\n",
    "- Doesnot rely on knowledge of environment dynamics\n",
    "- Agent interacts with environment\n",
    "- Learn policy thourgh trail and error\n",
    "- More suitable for real-world applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2e627",
   "metadata": {},
   "source": [
    "## **MONTE CARLO METHODS**\n",
    "- Model-free techniques\n",
    "- Estimate Q values based on episodes.\\\n",
    "**Process**\\\n",
    "*Collecting random episodes* &rarr; *Estimate Q-values using MV* &rarr; *Optimal policy*\n",
    "* Two methods:first visit, every vist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a3d9e",
   "metadata": {},
   "source": [
    "**Coustom grid world**\n",
    "![image](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862bf859",
   "metadata": {},
   "source": [
    "Suppose we collect two random episodes where states, actions, and rewards are as shown. Also, returns are computed for every state-action pair, considering a discount factor of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e3f04",
   "metadata": {},
   "source": [
    "We fill the Q-table, a table containing Q-values for state-action pairs. To fill a specific pair, we look for instances of this pair in the collected episodes and average them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50040927",
   "metadata": {},
   "source": [
    "Q(4, left), Q(4, up), and Q(1, down)\\\n",
    "(4, left), (4, up), and (1, down) occur once in an episode, therefore, we fill the Q-table with their returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf329e6",
   "metadata": {},
   "source": [
    "(4, Right) occurs once in each episode. We calculate its Q-value averaging the returns from both episodes, which gives a Q-value of 10. The distinction between first-visit and every-visit Monte Carlo emerges in handling repeated state-action pairs within episodes. (3, Right) for instance appears twice in the first episode and once in the second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db250ae",
   "metadata": {},
   "source": [
    "First-visit Monte Carlo averages only the first occurrence in every episode, leading to a Q-value of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a307ec",
   "metadata": {},
   "source": [
    "Every-visit Monte Carlo averages the returns of every occurrence, giving a Q-value of 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64b384",
   "metadata": {},
   "source": [
    "![image](2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b533f04",
   "metadata": {},
   "source": [
    "```\n",
    "def generate_episode():\n",
    "    episode=[]\n",
    "    state,info=env.reset()\n",
    "    terminated=False\n",
    "    while not terminated:\n",
    "        action=env.action_space.sample()\n",
    "        next_state,reward,terminated,truncated,info=env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state=next_state\n",
    "    return episode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91172a1f",
   "metadata": {},
   "source": [
    "```\n",
    "def fitst_vist_mc(num_episodes):\n",
    "    Q=np.zeros((num_states,num_actions))\n",
    "    returns_sum=np.zeros((num_states,num_actions))\n",
    "    returns_count=np.zeros((num_states,num_actions))\n",
    "    for i in range(num_episodes):\n",
    "        episode=generate_episode()\n",
    "        visited_states_actions=set()\n",
    "\n",
    "        for j,(state,action,reward) in enumerate(episode):\n",
    "            if (state,action) not in visited_states:\n",
    "                returns_sum[state,action] +=sum([x[2] for x in episodes[j:]])\n",
    "                returns_count[state,action]+=1\n",
    "                visted_states_actions.add((state,action))\n",
    "    \n",
    "    nonzero_counts =returns_count !=0\n",
    "    Q[nonzero_counts] =returns_sum[nonzero_counts]/returns_count[nonzero_counts]\n",
    "    return Q\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439dfe45",
   "metadata": {},
   "source": [
    "```\n",
    "def every_vist_mc(nun_episodes):\n",
    "    Q=np.zeros((num_states,num_actions))\n",
    "    returns_sum=np.zeros((num_states,num_actions))\n",
    "    returns_count=np.zeros((num_states,num_actions))\n",
    "    for i in range(num_episodes):\n",
    "        episode=generate_episode()\n",
    "        for j,(state,action,reward) in enumerate(episode):  \n",
    "            returns_sum[state,action] +=sum([x[2] for x in episodes[j:]])\n",
    "            returns_count[state,action]+=1\n",
    "    \n",
    "    nonzero_counts =returns_count !=0\n",
    "    Q[nonzero_counts] =returns_sum[nonzero_counts]/returns_count[nonzero_counts]\n",
    "    return Q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95408441",
   "metadata": {},
   "source": [
    "```\n",
    "def get_policy():\n",
    "    policy={state:np.argmax(Q[state]) for state in range(num_states)}\n",
    "    return policy\n",
    "\n",
    "Q=first_vist_mc(1000)\n",
    "policy_first_vist=get_policy()\n",
    "print('First_visited poilicy:\\n',policy_first_vist)\n",
    "Q=every_vist_mc(1000)\n",
    "policy_every_vist=get_policy()\n",
    "print('Every_visited poilicy:\\n',policy_every_vist)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b62c81",
   "metadata": {},
   "source": [
    "For most environments, both methods converge to the same Q-values and optimal policy as the number of episodes increases.\n",
    "For a small number of episodes or environments with many repeated visits, the Q-values and resulting policies may differ slightly.\n",
    "Every-visit often uses more data per episode, so it may converge faster in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e15efd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
